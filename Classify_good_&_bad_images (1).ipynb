{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9gaOtROBauJ",
        "outputId": "24e190b2-77ac-45c7-8fb0-369cbd092bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Verifying dataset structure...\n",
            "Good images folder: /content/drive/MyDrive/anomaly_detection_test_data/good\n",
            "Bad images folder: /content/drive/MyDrive/anomaly_detection_test_data/bad\n",
            "\n",
            "Sample 'good' images:\n",
            "['24_08_2024_18_12_36.034849_cls_input.png', '24_08_2024_18_07_39.184533_cls_input.png', '14_10_2024_13_04_36.123688_cls_input.png', '24_08_2024_18_09_22.348763_cls_input.png', '16_08_2024_12_37_56.690897_classifier_input.png']\n",
            "\n",
            "Sample 'bad' images:\n",
            "['03_08_2024_17_12_41.304965_classifier_input.png', 'Code03117.png', 'Code02553.png', '09_08_2024_18_36_59.620468_classifier_input.png', '16_08_2024_16_51_27.973379_classifier_input.png']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Corrected folder paths\n",
        "dataset_path = '/content/drive/MyDrive/anomaly_detection_test_data'\n",
        "\n",
        "good_images_path = os.path.join(dataset_path, 'good')\n",
        "bad_images_path = os.path.join(dataset_path, 'bad')\n",
        "masks_path = os.path.join(dataset_path, 'masks')\n",
        "\n",
        "# Verify dataset structure again\n",
        "print(\"Verifying dataset structure...\")\n",
        "print(f\"Good images folder: {good_images_path}\")\n",
        "print(f\"Bad images folder: {bad_images_path}\")\n",
        "\n",
        "# List files in good and bad image folders\n",
        "print(\"\\nSample 'good' images:\")\n",
        "print(os.listdir(good_images_path)[:5])\n",
        "\n",
        "print(\"\\nSample 'bad' images:\")\n",
        "print(os.listdir(bad_images_path)[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preproccessing"
      ],
      "metadata": {
        "id": "QT_jdeIqCpo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define preprocessing transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Testing the preprocessing on a sample image\n",
        "from PIL import Image\n",
        "\n",
        "# Load a sample image\n",
        "sample_image_path = os.path.join(good_images_path, os.listdir(good_images_path)[0])\n",
        "image = Image.open(sample_image_path).convert('RGB')\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_image = image_transforms(image)\n",
        "\n",
        "print(f\"Original Image Size: {image.size}\")\n",
        "print(f\"Preprocessed Image Shape: {preprocessed_image.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfufy4LWCUyM",
        "outputId": "5f87ba7e-c67d-4b8d-b856-3726cd59e425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Image Size: (333, 138)\n",
            "Preprocessed Image Shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to calculate Mean and STD fo this dataset"
      ],
      "metadata": {
        "id": "0u6Np5ZiDvPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to calculate mean and std\n",
        "def calculate_mean_std(image_paths):\n",
        "    pixel_sum = np.zeros(3)\n",
        "    pixel_squared_sum = np.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for image_path in tqdm(image_paths, desc=\"Calculating mean and std\"):\n",
        "        try:\n",
        "            # Open image and convert to RGB\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = np.array(image) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "            # Sum of pixels across channels\n",
        "            pixel_sum += image.sum(axis=(0, 1))\n",
        "            pixel_squared_sum += (image ** 2).sum(axis=(0, 1))\n",
        "\n",
        "            # Total number of pixels\n",
        "            num_pixels += image.shape[0] * image.shape[1]\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {image_path}: {e}\")\n",
        "\n",
        "    # Mean and std calculation\n",
        "    mean = pixel_sum / num_pixels\n",
        "    std = np.sqrt(pixel_squared_sum / num_pixels - mean ** 2)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# Get all valid image paths\n",
        "def get_valid_image_paths(folder_path, valid_extensions=('.jpg', '.jpeg', '.png')):\n",
        "    return [os.path.join(folder_path, img) for img in os.listdir(folder_path) if img.lower().endswith(valid_extensions)]\n",
        "\n",
        "# Get all image paths\n",
        "good_image_paths = get_valid_image_paths(good_images_path)\n",
        "bad_image_paths = get_valid_image_paths(bad_images_path)\n",
        "all_image_paths = good_image_paths + bad_image_paths\n",
        "\n",
        "# Calculate mean and std\n",
        "mean, std = calculate_mean_std(all_image_paths)\n",
        "\n",
        "print(f\"Calculated Mean: {mean}\")\n",
        "print(f\"Calculated Std: {std}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkT31TouDwKA",
        "outputId": "d1c3d2b7-3317-469f-d09c-a2080a82a236"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating mean and std: 100%|██████████| 5180/5180 [09:00<00:00,  9.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Mean: [0.7342358 0.7342358 0.7342358]\n",
            "Calculated Std: [0.33371068 0.33371068 0.33371068]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code For Preprocessing"
      ],
      "metadata": {
        "id": "U_Qn2ICyD6cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define preprocessing transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
        "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    transforms.Normalize(mean=mean, std=std)  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Testing the preprocessing on a sample image\n",
        "from PIL import Image\n",
        "\n",
        "# Load a sample image\n",
        "sample_image_path = os.path.join(good_images_path, os.listdir(good_images_path)[0])\n",
        "image = Image.open(sample_image_path).convert('RGB')\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocessed_image = image_transforms(image)\n",
        "\n",
        "print(f\"Original Image Size: {image.size}\")\n",
        "print(f\"Preprocessed Image Shape: {preprocessed_image.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG461zVYD6sq",
        "outputId": "3e2b661f-55e3-4277-b4a3-23adaf52a4b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Image Size: (333, 138)\n",
            "Preprocessed Image Shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom dataset class"
      ],
      "metadata": {
        "id": "q7WbgD1cEUYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageClassificationDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_paths (list): List of image file paths.\n",
        "            labels (list): Corresponding labels for each image.\n",
        "            transform (callable, optional): Transformation to apply to the images.\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get the label\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "Zw7fH5SOEUqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset spilitng and Dataset Loader Preparation"
      ],
      "metadata": {
        "id": "72CiK-1lLZIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define transformations\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=mean, std=std) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Prepare image paths and labels\n",
        "good_image_paths = get_valid_image_paths(good_images_path)\n",
        "bad_image_paths = get_valid_image_paths(bad_images_path)\n",
        "\n",
        "all_image_paths = good_image_paths + bad_image_paths\n",
        "all_labels = [0] * len(good_image_paths) + [1] * len(bad_image_paths)  # 0 for good, 1 for bad\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train (70%) and temp (30%)\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    all_image_paths, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
        ")\n",
        "\n",
        "# Split temp (30%) into validation (15%) and test (15%)\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "# Print sizes to verify\n",
        "print(f\"Training set size: {len(train_paths)}\")\n",
        "print(f\"Validation set size: {len(val_paths)}\")\n",
        "print(f\"Testing set size: {len(test_paths)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ImageClassificationDataset(train_paths, train_labels, transform=image_transforms)\n",
        "val_dataset = ImageClassificationDataset(val_paths, val_labels, transform=image_transforms)\n",
        "test_dataset = ImageClassificationDataset(test_paths, test_labels, transform=image_transforms)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Test the DataLoaders\n",
        "for images, labels in train_loader:\n",
        "    print(f\"Batch of images shape: {images.shape}\")\n",
        "    print(f\"Batch of labels shape: {labels.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2oXaTD5LY5T",
        "outputId": "1a277fc7-b53c-4a0b-8570-b4251e2132ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3626\n",
            "Validation set size: 777\n",
            "Testing set size: 777\n",
            "Batch of images shape: torch.Size([32, 3, 224, 224])\n",
            "Batch of labels shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Setup"
      ],
      "metadata": {
        "id": "68zZ4F1DOKZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer for binary classification\n",
        "num_features = model.fc.in_features  # Get the number of input features for the final FC layer\n",
        "model.fc = nn.Linear(num_features, 2)  # Replace with a new layer with 2 output classes (good/bad)\n",
        "\n",
        "# Move the model to the correct device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the modified model\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnL04h0jOMFn",
        "outputId": "26f91aa7-d1ec-4ca4-afb9-1f5d4590a419"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code For training setup"
      ],
      "metadata": {
        "id": "DbUdoailOOKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Define loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    _, predicted_classes = torch.max(predictions, 1)\n",
        "    correct = (predicted_classes == labels).sum().item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "Nc6b-CvqOOef"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code For Training Loop"
      ],
      "metadata": {
        "id": "9v1H2eq1OdVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    _, predicted_classes = torch.max(predictions, 1)\n",
        "    correct = (predicted_classes == labels).sum().item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n",
        "\n",
        "# Training loop\n",
        "best_val_accuracy = 0.0  # Track the best validation accuracy\n",
        "save_path = \"best_model.pth\"  # Specify where to save the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate training loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_classes = torch.max(outputs, 1)\n",
        "        train_correct += (predicted_classes == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "            val_correct += (predicted_classes == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Best model saved with Validation Accuracy: {best_val_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNZyuPb8OikP",
        "outputId": "41abd2da-b22c-4850-e22c-5d60a8ddc273"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:22<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1589, Training Accuracy: 0.9418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:03<00:00,  6.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0365, Validation Accuracy: 0.9910\n",
            "Best model saved with Validation Accuracy: 0.9910\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:23<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0302, Training Accuracy: 0.9920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:03<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0330, Validation Accuracy: 0.9897\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:22<00:00,  5.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0173, Training Accuracy: 0.9948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0265, Validation Accuracy: 0.9923\n",
            "Best model saved with Validation Accuracy: 0.9923\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:21<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0089, Training Accuracy: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:04<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0344, Validation Accuracy: 0.9923\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:23<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0306, Training Accuracy: 0.9928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:03<00:00,  6.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0298, Validation Accuracy: 0.9910\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:22<00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0288, Training Accuracy: 0.9906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0414, Validation Accuracy: 0.9897\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:21<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0042, Training Accuracy: 0.9994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:04<00:00,  5.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0247, Validation Accuracy: 0.9949\n",
            "Best model saved with Validation Accuracy: 0.9949\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:24<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0116, Training Accuracy: 0.9959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:03<00:00,  6.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0282, Validation Accuracy: 0.9923\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:23<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0048, Training Accuracy: 0.9986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:03<00:00,  6.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0278, Validation Accuracy: 0.9910\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 114/114 [00:21<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0010, Training Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:05<00:00,  4.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0204, Validation Accuracy: 0.9949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "sIvLrPoYSuNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted_classes == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_accuracy = correct_predictions / total_samples\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Run testing\n",
        "test_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fZ-ONlrS0SI",
        "outputId": "7fd7820d-cf68-4ff6-d142-e61991fa081b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 25/25 [00:04<00:00,  6.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0372, Test Accuracy: 0.9910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the testing model"
      ],
      "metadata": {
        "id": "lF1c3IPlS8-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"resnet18_good_bad_classification.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slitptLIS9XF",
        "outputId": "8fa72846-340c-4f5a-afaa-a6bf87a27165"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and use the trained model"
      ],
      "metadata": {
        "id": "rPbWA0AFTsIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(num_features, 2)  # Ensure the same structure as trained model\n",
        "model.load_state_dict(torch.load(\"resnet18_good_bad_classification.pth\"))\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Predict on a new image\n",
        "from PIL import Image\n",
        "new_image_path = \"/content/drive/MyDrive/anomaly_detection_test_data/bad/Code03775.png\"  # Replace with the actual path\n",
        "image = Image.open(new_image_path).convert('RGB')\n",
        "\n",
        "# Preprocess the image\n",
        "preprocessed_image = image_transforms(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Perform prediction\n",
        "with torch.no_grad():\n",
        "    output = model(preprocessed_image)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "    print(f\"Predicted Class: {'Good' if predicted_class.item() == 0 else 'Bad'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VuX5TcbTn6M",
        "outputId": "e6ac91aa-6ee6-400b-87ad-70ce3ea29b35"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Bad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b5fcc4357b73>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"resnet18_good_bad_classification.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LK8S1r7mT0HK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}